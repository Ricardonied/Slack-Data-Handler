# Install and load required libraries
# install.packages(c("dplyr", "tidytext", "tm", "tidymodels", "stm", "reshape2", "forcats", "knitr", "ggplot2", "pdftools", "wordcloud", "broom"))
library(dplyr)
library(tidytext)
library(tm)
library(tidymodels)
library(stm)
library(reshape2)
library(forcats)
library(knitr)
library(ggplot2)
library(pdftools)

# Directory containing the PDFs generated by the export script
pdf_directory <- path.expand("~/Documents/project_pdfs")
pdf_files <- list.files(pdf_directory, pattern = "\.pdf$", full.names = TRUE)

# Extract text from the PDFs
pdf_texts <- lapply(pdf_files, pdf_text)


# Build initial data frame
num_channels <- length(pdf_files)
id <- paste0("doc", 1:num_channels)

channel_name <- tools::file_path_sans_ext(basename(pdf_files))


# Combine the pieces into a data frame
channel_data <- data.frame(id = character(length(pdf_texts)), channel_name = character(length(pdf_texts)), text = character(length(pdf_texts)), stringsAsFactors = FALSE)

for (i in seq_along(pdf_texts)) {
  channel_data[i, "id"] <- id[i]
  channel_data[i, "channel_name"] <- channel_name[i]
  channel_data[i, "text"] <- paste(pdf_texts[[i]], collapse = " ")
}

# Load custom stop words
custom_stopwords_path <- path.expand("~/Desktop/custom_stopwords.txt")
custom_stopwords <- readLines(custom_stopwords_path)

custom_stopwords <- tibble(word = custom_stopwords)

custom_stopwords <- custom_stopwords %>%
  mutate(word = tolower(word)) %>%
  mutate(word = gsub("[[:punct:]]", "", word)) %>%
  mutate(word = gsub("[[:digit:]]", "", word)) %>%
  mutate(word = gsub("[^[:alnum:]\\s]", "", word))

### Tokenization and stopword removal
tidy_df <- channel_data %>%
  unnest_tokens(word, text) %>%
  anti_join(get_stopwords()) %>%
  anti_join(custom_stopwords)
tidy_df %>%
  count(word, sort = TRUE)

# Limpeza geral
tidy_df <- tidy_df %>%
  mutate(word = gsub("[[:punct:]]", "", word)) %>%
  mutate(word = tolower(word)) %>%
  mutate(word = gsub("[[:digit:]]", "", word)) %>%
  mutate(word = gsub("[^[:alnum:]\\s]", "", word)) %>%
  filter(nchar(word) >= 3)

# Reapply stopword removal after transformations
tidy_df <- tidy_df %>%
  anti_join(get_stopwords(), by = "word") %>%
  anti_join(custom_stopwords, by = "word")

# Remove punctuation, numbers, special characters and short words

# Lemmatization using textstem
library(textstem)
tidy_dfl <- tidy_df  %>%
  mutate(word = textstem::lemmatize_words(word, language = "en"))

tidy_dfl %>%
  count(word, sort = T)

## Descriptive analysis using tidytext
library(dplyr)
library(tidytext)

df <- tidy_dfl

word_counts <- df %>%
  count(word, sort = TRUE)

library(ggplot2)

# Word frequency bar chart
grafico_palavras <- word_counts %>%
  top_n(25) %>%
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(x = "Words", y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text = element_text(size = 12))
grafico_palavras

# Check document identifiers
df %>% distinct(channel_name)

## Topic modeling
library(tm)
library (tidytext)
library(tidymodels)

# Create the document-term matrix
spars_tidy_dfl <- tidy_dfl %>%
  count(channel_name, word) %>%
  cast_sparse(channel_name, word, n)

spars_tidy_dfl

# Train a model
library(stm)
# For small corpora we will use k = 5
topic_model <- stm(spars_tidy_dfl, K = 5)

# Check model summary
summary(topic_model)

score_words <- labelTopics(topic_model, n = 15, topics = 1:5)

score_words

# Probability of occurrence per topic
# reshape2 and broom may be required for visualizations
library(reshape2)
library(broom)
library(tidytext)

canais_topic <- tidy(topic_model, matrix = "beta")

library(forcats)
library(knitr)
library(dplyr)

canais_topic <- canais_topic %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  mutate(term = forcats::fct_reorder(term, beta), topic = paste("Topic", topic))
print(canais_topic, n = 50)
knitr::kable(canais_topic)

library(ggplot2)
canais_topic <- tidy(topic_model, matrix = "beta")
canais_topic %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  mutate(term = forcats::fct_reorder(term, beta), topic = paste("Topic", topic)) %>%
  ggplot(aes(beta, term, fill = topic))+
  geom_col(show.legend = F)+
  facet_wrap(vars(topic), scales = "free_y")+
  labs(x = expression(beta), y = NULL)


# Plot ordered topic probabilities

canais_topic <- tidy(topic_model, matrix = "beta")
canais_topic %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  mutate(topic = paste ("Topic", topic)) %>%
  ggplot(aes(beta, reorder_within(term, beta, topic), fill = topic))+
  geom_col(show.legend = F)+
  facet_wrap(vars(topic), scales = "free_y")+
  scale_y_reordered()+
  labs(x = expression(beta), y = NULL)

# Probabilidade por documento
doc_topic <- tidy(topic_model, matrix = "gamma",document_names = rownames(spars_tidy_dfl))
print(doc_topic, n = 215)

# Identificando um tÃ³pico para cada documento
doc_gg <- doc_topic %>%
  mutate(document = forcats::fct_reorder(document, gamma), topic = factor(topic)) %>%
  ggplot(aes(gamma, topic, fill = topic))+
  geom_col(show.legend = F)+
  facet_wrap(vars(document))+
  theme(strip.text = element_text(size = 7))+
  labs(x = expression(gamma), y = "Topic")

ggsave(path.expand("~/Desktop/gamma_plot.png"), doc_gg, width = 11, height = 8.5, dpi = 300)

# View the data as a table instead of a plot
doc_topic_t <- doc_topic %>%
  pivot_wider(names_from = topic, values_from = gamma, values_fill = 0)
doc_topic_t


# Additional visualisations

# Most frequent words
word_counts <- tidy_dfl %>%
  count(word, sort = TRUE)

head(word_counts, 10)

ggplot(word_counts %>% top_n(25), aes(x = reorder(word, n), y = n)) +
  geom_bar(stat = "identity", fill = "blue") +
  coord_flip() +
  labs(x = "Words", y = "Frequency") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Word cloud
library(wordcloud)
library(RColorBrewer)
wordcloud(words = word_counts$word, freq = word_counts$n, min.freq = 2,
          random.order = FALSE, colors = brewer.pal(8, "Dark2"))

