# Script for performing topic modelling on the PDFs produced by the
# `slack_export` script. It cleans the text, removes stopwords and
# builds a Structural Topic Model to visualise the main themes.

# Install and load required packages
install.packages(c("dplyr", "tidytext", "tm", "tidymodels", "stm", "reshape2", "forcats", "knitr", "ggplot2"))
library(dplyr)
library(tidytext)
library(tm)
library(tidymodels)
library(stm)
library(reshape2)
library(forcats)
library(knitr)
library(ggplot2)
install.packages("pdftools")
library(pdftools)

# Define the directory where the PDFs are located and list them
diretorio <- "C:/Users/Usuário/Documents/projeto_pdfs"
arquivos_pdf <- list.files(diretorio, pattern = ".pdf$", full.names = TRUE)

# Extract text content from each PDF file
texto_pdf <- lapply(arquivos_pdf, pdf_text)

# Build the base data frame

# Variable 1: an id for each channel
num_canais <- length(arquivos_pdf)
id <- paste0("art", 1:num_canais)

# Variable 2: channel name derived from file names
nomecanal <- tools::file_path_sans_ext(basename(arquivos_pdf))


# Combine everything into a data frame
# Each list item has a different length, so `cbind` or `data.frame`
# directly would not work.

# Create an empty data frame
dados_canal <- data.frame(id = character(length(texto_pdf)), nomecanal = character(length(texto_pdf)), texto = character(length(texto_pdf)), stringsAsFactors = FALSE)

# Loop over the PDFs assigning id, name and text
for (i in 1:length(texto_pdf)) {
  dados_canal[i, "id"] <- id[i]
  dados_canal[i, "nomecanal"] <- nomecanal[i]
  dados_canal[i, "texto"] <- paste(texto_pdf[[i]], collapse = " ")
}

# Load a file with custom stopwords
caminho_arquivo <- "C:/Users/Usuário/Desktop/stopwords_personalizadas.txt" 
stopwords_personalizadas <- readLines(caminho_arquivo)

# Convert to a tibble
stopwords_personalizadas <- tibble(word = stopwords_personalizadas)

stopwords_personalizadas <- stopwords_personalizadas %>%
  mutate(word = tolower(word)) %>%
  mutate(word = gsub("[[:punct:]]", "", word)) %>%
  mutate(word = gsub("[[:digit:]]", "", word)) %>%
  mutate(word = gsub("[^[:alnum:]\\s]", "", word))

### Tokenisation, removal of stopwords and custom stopwords
tidy_df <- dados_canal %>%
  unnest_tokens(word, texto) %>%
  anti_join(get_stopwords()) %>%
  anti_join(stopwords_personalizadas) 
tidy_df %>%
  count(word, sort = T)

# General cleaning steps
tidy_df <- tidy_df %>%
  mutate(word = gsub("[[:punct:]]", "", word)) %>%
  mutate(word = tolower(word)) %>%
  mutate(word = gsub("[[:digit:]]", "", word)) %>%
  mutate(word = gsub("[^[:alnum:]\\s]", "", word)) %>%
  filter(nchar(word) >= 3)

# Reapply stopword removal after transformations
tidy_df <- tidy_df %>%
  anti_join(get_stopwords(), by = "word") %>%
  anti_join(stopwords_personalizadas, by = "word")

# Remove punctuation
# Convert to lowercase to avoid duplicates
# Remove numbers and special characters
# Drop very short words when possible

# Lemmatization - reduce words to their base form using textstem
install.packages("textstem")
library(textstem)
tidy_dfl <- tidy_df  %>%
  mutate(word = textstem::lemmatize_words(word, language = "en"))

tidy_dfl %>%
  count(word, sort = T)

## Basic descriptive analysis using tidytext
library(dplyr)
library(tidytext)

df <- tidy_dfl

contagem_palavras <- df %>%
  count(word, sort=T)

library(ggplot2)

# Plot a simple word frequency bar chart (similar to a word cloud)
grafico_palavras <- contagem_palavras %>%
  top_n(25) %>%
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_bar(stat = "identity", fill = "blue") +  # Change bar colour if desired
  labs(x = "Palavras", y = "Contagem") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text = element_text(size = 12))
grafico_palavras

# Check the column containing document identifiers
df %>% distinct(nomecanal)

## Topic modelling
library(tm)
library (tidytext)
library(tidymodels)

# Create a sparse matrix with term counts per channel
spars_tidy_dfl <- tidy_dfl %>%
  count(nomecanal, word) %>%
  cast_sparse(nomecanal, word, n)

spars_tidy_dfl

# Train the STM model
library(stm)
# For small datasets we'll keep k = 5 topics
topic_model <- stm(spars_tidy_dfl, K = 5)

# Inspect the summary
summary(topic_model)

score_words <- labelTopics(topic_model, n=15, topics = 1:5)

score_words

# Word probabilities per topic
install.packages("reshape2")
library(reshape2)
install.packages("broom")
library(broom)
library(tidytext)

canais_topic <- tidy(topic_model, matrix = "beta")

install.packages("forcats")
library(forcats)
install.packages("knitr")
library(knitr)
library(dplyr)

canais_topic <- canais_topic %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  mutate(term = forcats::fct_reorder(term, beta), topic = paste("Topic", topic))
print(canais_topic, n = 50)
knitr::kable(canais_topic)

library(ggplot2)
canais_topic <- tidy(topic_model, matrix = "beta")
canais_topic %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  mutate(term = forcats::fct_reorder(term, beta), topic = paste("Topic", topic)) %>%
  ggplot(aes(beta, term, fill = topic))+
  geom_col(show.legend = F)+
  facet_wrap(vars(topic), scales = "free_y")+
  labs(x = expression(beta), y = NULL)


# Plot again with ordered probabilities

canais_topic <- tidy(topic_model, matrix = "beta")
canais_topic %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>%
  ungroup() %>%
  mutate(topic = paste ("Topic", topic)) %>%
  ggplot(aes(beta, reorder_within(term, beta, topic), fill = topic))+
  geom_col(show.legend = F)+
  facet_wrap(vars(topic), scales = "free_y")+
  scale_y_reordered()+
  labs(x = expression(beta), y = NULL)

# Document-topic probabilities
doc_topic <- tidy(topic_model, matrix = "gamma",document_names = rownames(spars_tidy_dfl))
print(doc_topic, n = 215)

# Identify one topic for each document
doc_gg <- doc_topic %>%
  mutate(document = forcats::fct_reorder(document, gamma), topic = factor(topic)) %>%
  ggplot(aes(gamma, topic, fill = topic))+
  geom_col(show.legend = F)+
  facet_wrap(vars(document))+
  theme(strip.text = element_text(size = 7))+
  labs(x = expression(gamma), y = "Topic")

ggsave("C:/Users/Usuário/Desktop/gamma_tinst_p.png", doc_gg, width = 11, height = 8.5, dpi = 300)

# View in table form instead of a plot
doc_topic_t <- doc_topic %>%
  pivot_wider(names_from = topic, values_from = gamma, values_fill = 0)
doc_topic_t


# Other visualisations

# Show the 10 most frequent words
contagem_palavras <- tidy_dfl %>%
  count(word, sort = TRUE)

head(contagem_palavras, 10)

# Bar plot of the most frequent words
ggplot(contagem_palavras %>% top_n(25), aes(x = reorder(word, n), y = n)) +
  geom_bar(stat = "identity", fill = "blue") +
  coord_flip() +
  labs(x = "Palavras", y = "Frequência") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Word cloud of the terms
install.packages("wordcloud")
library(wordcloud)
library(RColorBrewer)
wordcloud(words = contagem_palavras$word, freq = contagem_palavras$n, min.freq = 2,
          random.order = FALSE, colors = brewer.pal(8, "Dark2"))

